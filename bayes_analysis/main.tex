% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
]{article}
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother





\setlength{\emergencystretch}{3em} % prevent overfull lines

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



 



\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Variance All the Way Down: Exploring the Impact of RNA-Seq Pipeline Choices on Differential Expression Variance},
  pdfauthor={Art Tay and Hunter Schuler},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{\textbf{Variance All the Way Down:} Exploring the Impact of
RNA-Seq Pipeline Choices on Differential Expression Variance}
\author{Art Tay and Hunter Schuler}
\date{}
\begin{document}
\maketitle


\section{Data}\label{data}

We first pull raw RNA-Seq reads for the ovarian cancer study
\href{https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE282674}{GSE282674}
from the NCBI-NIH sequence read archive (SRA). This dataset contains 9
samples with 3 in the treatment group. The reads are also single-ended.
The data was extracted using the default \texttt{prefetch} settings in
the \texttt{sra-toolkit}. We then generated \texttt{fastq} files using
\texttt{fasterq-dump}. We initially varied the parameters, but found no
impact on the resulting \texttt{fastq} files. We then applied
\texttt{fastp} to filter out low quality reads and trim excess bases.
After that, the reads were aligned and the genes counted. At this stage
we save the resulting count matrices under the various aligners and
\texttt{fastp} parameters, so that they can be compared to the official
count matrix posted on the archive. After this, we used \texttt{edgeR}
to normalize the counts and compute p-values and effect sizes. Baseline
p-values and effect sizes were computing using the official count matrix
and no count normalization. See Table~\ref{tbl-1} for pipeline
parameters.

\begin{longtable}[t]{llll}

\caption{\label{tbl-1}Basic RNA-Seq Differential Analysis End-to-End
Pipeline}

\tabularnewline

\toprule
Pipeline Steps & Software & Options & Choices\\
\midrule
1. Pull SRA data from the NIH. & prefetch & NA & NA\\
 &  &  & \\
2. Compute quality scores. & fasterq-dump & NA & NA\\
 &  &  & \\
3. Filter low quality reads. & fastp & --qualified\_quality\_phred X & 20-30\\
 &  & --length\_required X & 30-50\\
 &  &  & \\
4. Trim excess bases. & fastp & --trim\_poly\_g & 1 or 0\\
 &  & --trim\_ploy\_x & 1 or 0\\
 &  &  & \\
5. Align and count genes. & Various & Default & Salmon, Kallisto\\
 &  &  & STAR, HISAT2\\
 &  &  & \\
6. Count normalization. & edgeR & calcNormFactors(method='X') & TMM, TMMwsp, RLE\\
 &  &  & upperquartile\\
 &  &  & ALDEx2, none\\
 &  &  & \\
7. Differential expression analysis. & edgeR & Default & NA\\
\bottomrule

\end{longtable}

We are interested in the effect of pipeline choices on the variation of
3 quantities: counts, p-values, and effect sizes. To quantify variation
we compute the average sum of squared differences between pipeline
outputs and the NIH baseline. Assume that there are \(G\) genes. Let
\(A_{gX}\) denote the resulting outcome of pipeline \(X\), and let
\(B_{g}\) denote the corresponding result based on the official count
matrix. Now we can define the form of our target variable:\\
\begin{equation}
    Y^2 = \dfrac{1}{G} \sum_{g=1}^G (A_{gX} - B_{g})^2 
\end{equation} Note that the true variable of interest is \(Y\), since
it will be on the same scale as the underlying variable.

\section{Models}\label{models}

We propose a Bayesian hierarchical model building from the fact that
\(Y \geq 0\). \begin{equation}
\begin{aligned} 
    Y_i &\sim f_{Y_i}(y) \quad s.t. \quad Y_i \geq 0 \\ 
    \mathbb{E} \ Y_i &= \exp(X_i \beta) 
        \quad \text{since} \quad \mathbb{E} \ Y_i \geq 0 \\ 
    \mathbb{V} \ Y_i &= a \cdot \mu_i^b \\
    \beta &\sim \mathcal{N}(0, \sigma^2_\beta) \\
    a &\sim \text{Gamma}(c, d) \\ 
    b &\sim \mathcal{N}(0, \sigma^2_b)
\end{aligned}
\end{equation} This model has a couple of key advantages: 1. Assumption
are explicit, but variable.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Natural parameter shrinkage via the prior on \(\beta\). Handles
  multicollinearity and high dimensionality of \(X\).
\item
  Does not assume constant variance. Specifically, we are applying the
  variance-power law from the Tweedie family of distributions
  {[}cite{]}. \begin{equation}
   \text{Var} \ Y \propto (\mathbb{E} \ Y)^p 
  \end{equation} \(a > 0\) and represents a common variance scale ie if
  \(b = 0\) we recover the classical log-Normal regression model.
  \(b \in \mathbb{R}\) where \(b > 0\) indicates over-dispersion and
  \(b < 0\) indicates under-dispersion.
\item
  Clear interpretation and interval estimation.
\item
  Clear model diagnostic checks.
\end{enumerate}

Pre-specified Model Settings:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \(f_{Y_i}(y)\) is log-Normal, Gamma, or Weibull.
\item
  \(\sigma^2_\beta=100 \Rightarrow\) flat prior.
\item
  \(c = d = 0.01 \Rightarrow\) flat prior.
\item
  \(\sigma^2_b = 1\). High shrinkage because we do not expect crazy over
  dispersion.
\end{enumerate}

Pre-specified Model Validation:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Convergence - \(\hat R \leq 1.1\)
\item
  Fit - Posterior Predictive Distribution Quantiles (10\%)
\item
  General Stan warnings.
\end{enumerate}

Decision Criteria:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Assuming valid fit a convergence, we will base the ``significance'' of
  a predictor based on a 95\% credible interval.
\item
  Agreement across likelihood is seen as stronger evidence.
\end{enumerate}

\section{Results}\label{results}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{count\_sd\_df\_clean }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"./data/count\_sd\_df\_clean.csv"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(cmdstanr)}
\CommentTok{\# cmdstanr::install\_cmdstan() \# One time. }

\FunctionTok{options}\NormalTok{(}\AttributeTok{mc.cores =}\NormalTok{ parallel}\SpecialCharTok{::}\FunctionTok{detectCores}\NormalTok{())}

\NormalTok{n\_chains }\OtherTok{\textless{}{-}} \DecValTok{2}
\NormalTok{n\_iter }\OtherTok{\textless{}{-}} \DecValTok{500}
\NormalTok{burn\_in }\OtherTok{\textless{}{-}} \DecValTok{100}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lik }\OtherTok{\textless{}{-}} \DecValTok{1} \CommentTok{\#1=log{-}Normal, 2=Gamma, 3=Weibull}
\NormalTok{sigma\_beta }\OtherTok{\textless{}{-}} \DecValTok{100}
\NormalTok{c }\OtherTok{\textless{}{-}} \FloatTok{0.01}\NormalTok{; d }\OtherTok{\textless{}{-}} \FloatTok{0.01}
\NormalTok{sigma\_b }\OtherTok{\textless{}{-}} \DecValTok{1}

\NormalTok{data }\OtherTok{\textless{}{-}}\NormalTok{ count\_sd\_df\_clean}
\NormalTok{X }\OtherTok{\textless{}{-}}\NormalTok{ data }\SpecialCharTok{|\textgreater{}} \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{count\_sd)}
\NormalTok{Y }\OtherTok{\textless{}{-}}\NormalTok{ data }\SpecialCharTok{|\textgreater{}} \FunctionTok{pull}\NormalTok{(count\_sd) }
\NormalTok{N }\OtherTok{\textless{}{-}} \FunctionTok{dim}\NormalTok{(X)[}\DecValTok{1}\NormalTok{]}
\NormalTok{P }\OtherTok{\textless{}{-}} \FunctionTok{dim}\NormalTok{(X)[}\DecValTok{2}\NormalTok{]}

\NormalTok{stan\_data\_list }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}
    \AttributeTok{N =}\NormalTok{ N, }\AttributeTok{P =}\NormalTok{ P, }\AttributeTok{X =}\NormalTok{ X, }\AttributeTok{Y =}\NormalTok{ Y, }
    \AttributeTok{lik =}\NormalTok{ lik, }\AttributeTok{sigma\_beta =}\NormalTok{ sigma\_beta, }\AttributeTok{c =}\NormalTok{ c, }\AttributeTok{d =}\NormalTok{ d, }\AttributeTok{sigma\_b =}\NormalTok{ sigma\_b}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model\_1 }\OtherTok{\textless{}{-}} \FunctionTok{cmdstan\_model}\NormalTok{(}\StringTok{"./model.stan"}\NormalTok{)}

\NormalTok{fit\_1 }\OtherTok{\textless{}{-}}\NormalTok{ model\_1}\SpecialCharTok{$}\FunctionTok{sample}\NormalTok{(}
    \AttributeTok{data =}\NormalTok{ stan\_data\_list, }
    \AttributeTok{chains =}\NormalTok{ n\_chains, }\AttributeTok{iter\_sampling =}\NormalTok{ n\_iter, }\AttributeTok{iter\_warmup =}\NormalTok{ burn\_in, }
    \AttributeTok{seed =} \DecValTok{04272025}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Running MCMC with 4 chains, at most 64 in parallel...
\end{verbatim}

\begin{verbatim}
Chain 1 Iteration:    1 / 6000 [  0%]  (Warmup) 
\end{verbatim}

\begin{verbatim}
Chain 2 Iteration:    1 / 6000 [  0%]  (Warmup) 
\end{verbatim}

\begin{verbatim}
Chain 3 Iteration:    1 / 6000 [  0%]  (Warmup) 
\end{verbatim}

\begin{verbatim}
Chain 4 Iteration:    1 / 6000 [  0%]  (Warmup) 
\end{verbatim}

\begin{verbatim}
Chain 2 Iteration:  100 / 6000 [  1%]  (Warmup) 
Chain 1 Iteration:  100 / 6000 [  1%]  (Warmup) 
Chain 4 Iteration:  100 / 6000 [  1%]  (Warmup) 
Chain 3 Iteration:  100 / 6000 [  1%]  (Warmup) 
Chain 2 Iteration:  200 / 6000 [  3%]  (Warmup) 
Chain 3 Iteration:  200 / 6000 [  3%]  (Warmup) 
Chain 4 Iteration:  200 / 6000 [  3%]  (Warmup) 
Chain 1 Iteration:  200 / 6000 [  3%]  (Warmup) 
Chain 2 Iteration:  300 / 6000 [  5%]  (Warmup) 
Chain 1 Iteration:  300 / 6000 [  5%]  (Warmup) 
Chain 3 Iteration:  300 / 6000 [  5%]  (Warmup) 
Chain 4 Iteration:  300 / 6000 [  5%]  (Warmup) 
Chain 2 Iteration:  400 / 6000 [  6%]  (Warmup) 
Chain 3 Iteration:  400 / 6000 [  6%]  (Warmup) 
Chain 4 Iteration:  400 / 6000 [  6%]  (Warmup) 
Chain 1 Iteration:  400 / 6000 [  6%]  (Warmup) 
Chain 2 Iteration:  500 / 6000 [  8%]  (Warmup) 
Chain 3 Iteration:  500 / 6000 [  8%]  (Warmup) 
Chain 4 Iteration:  500 / 6000 [  8%]  (Warmup) 
Chain 1 Iteration:  500 / 6000 [  8%]  (Warmup) 
Chain 2 Iteration:  600 / 6000 [ 10%]  (Warmup) 
Chain 3 Iteration:  600 / 6000 [ 10%]  (Warmup) 
Chain 4 Iteration:  600 / 6000 [ 10%]  (Warmup) 
Chain 1 Iteration:  600 / 6000 [ 10%]  (Warmup) 
Chain 2 Iteration:  700 / 6000 [ 11%]  (Warmup) 
Chain 3 Iteration:  700 / 6000 [ 11%]  (Warmup) 
Chain 4 Iteration:  700 / 6000 [ 11%]  (Warmup) 
Chain 1 Iteration:  700 / 6000 [ 11%]  (Warmup) 
Chain 2 Iteration:  800 / 6000 [ 13%]  (Warmup) 
Chain 3 Iteration:  800 / 6000 [ 13%]  (Warmup) 
Chain 1 Iteration:  800 / 6000 [ 13%]  (Warmup) 
Chain 4 Iteration:  800 / 6000 [ 13%]  (Warmup) 
Chain 2 Iteration:  900 / 6000 [ 15%]  (Warmup) 
Chain 3 Iteration:  900 / 6000 [ 15%]  (Warmup) 
Chain 4 Iteration:  900 / 6000 [ 15%]  (Warmup) 
Chain 1 Iteration:  900 / 6000 [ 15%]  (Warmup) 
Chain 2 Iteration: 1000 / 6000 [ 16%]  (Warmup) 
Chain 2 Iteration: 1001 / 6000 [ 16%]  (Sampling) 
Chain 3 Iteration: 1000 / 6000 [ 16%]  (Warmup) 
Chain 3 Iteration: 1001 / 6000 [ 16%]  (Sampling) 
Chain 4 Iteration: 1000 / 6000 [ 16%]  (Warmup) 
Chain 4 Iteration: 1001 / 6000 [ 16%]  (Sampling) 
Chain 1 Iteration: 1000 / 6000 [ 16%]  (Warmup) 
Chain 1 Iteration: 1001 / 6000 [ 16%]  (Sampling) 
Chain 2 Iteration: 1100 / 6000 [ 18%]  (Sampling) 
Chain 3 Iteration: 1100 / 6000 [ 18%]  (Sampling) 
Chain 4 Iteration: 1100 / 6000 [ 18%]  (Sampling) 
Chain 1 Iteration: 1100 / 6000 [ 18%]  (Sampling) 
Chain 2 Iteration: 1200 / 6000 [ 20%]  (Sampling) 
Chain 3 Iteration: 1200 / 6000 [ 20%]  (Sampling) 
Chain 4 Iteration: 1200 / 6000 [ 20%]  (Sampling) 
Chain 1 Iteration: 1200 / 6000 [ 20%]  (Sampling) 
Chain 2 Iteration: 1300 / 6000 [ 21%]  (Sampling) 
Chain 3 Iteration: 1300 / 6000 [ 21%]  (Sampling) 
Chain 4 Iteration: 1300 / 6000 [ 21%]  (Sampling) 
Chain 1 Iteration: 1300 / 6000 [ 21%]  (Sampling) 
Chain 2 Iteration: 1400 / 6000 [ 23%]  (Sampling) 
Chain 3 Iteration: 1400 / 6000 [ 23%]  (Sampling) 
Chain 4 Iteration: 1400 / 6000 [ 23%]  (Sampling) 
Chain 1 Iteration: 1400 / 6000 [ 23%]  (Sampling) 
Chain 2 Iteration: 1500 / 6000 [ 25%]  (Sampling) 
Chain 3 Iteration: 1500 / 6000 [ 25%]  (Sampling) 
Chain 1 Iteration: 1500 / 6000 [ 25%]  (Sampling) 
Chain 4 Iteration: 1500 / 6000 [ 25%]  (Sampling) 
Chain 2 Iteration: 1600 / 6000 [ 26%]  (Sampling) 
Chain 3 Iteration: 1600 / 6000 [ 26%]  (Sampling) 
Chain 1 Iteration: 1600 / 6000 [ 26%]  (Sampling) 
Chain 4 Iteration: 1600 / 6000 [ 26%]  (Sampling) 
Chain 2 Iteration: 1700 / 6000 [ 28%]  (Sampling) 
Chain 3 Iteration: 1700 / 6000 [ 28%]  (Sampling) 
Chain 4 Iteration: 1700 / 6000 [ 28%]  (Sampling) 
Chain 1 Iteration: 1700 / 6000 [ 28%]  (Sampling) 
Chain 2 Iteration: 1800 / 6000 [ 30%]  (Sampling) 
Chain 3 Iteration: 1800 / 6000 [ 30%]  (Sampling) 
Chain 4 Iteration: 1800 / 6000 [ 30%]  (Sampling) 
Chain 1 Iteration: 1800 / 6000 [ 30%]  (Sampling) 
Chain 2 Iteration: 1900 / 6000 [ 31%]  (Sampling) 
Chain 3 Iteration: 1900 / 6000 [ 31%]  (Sampling) 
Chain 4 Iteration: 1900 / 6000 [ 31%]  (Sampling) 
Chain 1 Iteration: 1900 / 6000 [ 31%]  (Sampling) 
Chain 2 Iteration: 2000 / 6000 [ 33%]  (Sampling) 
Chain 3 Iteration: 2000 / 6000 [ 33%]  (Sampling) 
Chain 1 Iteration: 2000 / 6000 [ 33%]  (Sampling) 
Chain 4 Iteration: 2000 / 6000 [ 33%]  (Sampling) 
Chain 2 Iteration: 2100 / 6000 [ 35%]  (Sampling) 
Chain 1 Iteration: 2100 / 6000 [ 35%]  (Sampling) 
Chain 3 Iteration: 2100 / 6000 [ 35%]  (Sampling) 
Chain 4 Iteration: 2100 / 6000 [ 35%]  (Sampling) 
Chain 2 Iteration: 2200 / 6000 [ 36%]  (Sampling) 
Chain 3 Iteration: 2200 / 6000 [ 36%]  (Sampling) 
Chain 1 Iteration: 2200 / 6000 [ 36%]  (Sampling) 
Chain 4 Iteration: 2200 / 6000 [ 36%]  (Sampling) 
Chain 2 Iteration: 2300 / 6000 [ 38%]  (Sampling) 
Chain 3 Iteration: 2300 / 6000 [ 38%]  (Sampling) 
Chain 1 Iteration: 2300 / 6000 [ 38%]  (Sampling) 
Chain 4 Iteration: 2300 / 6000 [ 38%]  (Sampling) 
Chain 2 Iteration: 2400 / 6000 [ 40%]  (Sampling) 
Chain 1 Iteration: 2400 / 6000 [ 40%]  (Sampling) 
Chain 3 Iteration: 2400 / 6000 [ 40%]  (Sampling) 
Chain 4 Iteration: 2400 / 6000 [ 40%]  (Sampling) 
Chain 2 Iteration: 2500 / 6000 [ 41%]  (Sampling) 
Chain 3 Iteration: 2500 / 6000 [ 41%]  (Sampling) 
Chain 1 Iteration: 2500 / 6000 [ 41%]  (Sampling) 
Chain 4 Iteration: 2500 / 6000 [ 41%]  (Sampling) 
Chain 2 Iteration: 2600 / 6000 [ 43%]  (Sampling) 
Chain 3 Iteration: 2600 / 6000 [ 43%]  (Sampling) 
Chain 1 Iteration: 2600 / 6000 [ 43%]  (Sampling) 
Chain 4 Iteration: 2600 / 6000 [ 43%]  (Sampling) 
Chain 2 Iteration: 2700 / 6000 [ 45%]  (Sampling) 
Chain 1 Iteration: 2700 / 6000 [ 45%]  (Sampling) 
Chain 3 Iteration: 2700 / 6000 [ 45%]  (Sampling) 
Chain 4 Iteration: 2700 / 6000 [ 45%]  (Sampling) 
Chain 1 Iteration: 2800 / 6000 [ 46%]  (Sampling) 
Chain 2 Iteration: 2800 / 6000 [ 46%]  (Sampling) 
Chain 3 Iteration: 2800 / 6000 [ 46%]  (Sampling) 
Chain 4 Iteration: 2800 / 6000 [ 46%]  (Sampling) 
Chain 2 Iteration: 2900 / 6000 [ 48%]  (Sampling) 
Chain 3 Iteration: 2900 / 6000 [ 48%]  (Sampling) 
Chain 1 Iteration: 2900 / 6000 [ 48%]  (Sampling) 
Chain 4 Iteration: 2900 / 6000 [ 48%]  (Sampling) 
Chain 3 Iteration: 3000 / 6000 [ 50%]  (Sampling) 
Chain 1 Iteration: 3000 / 6000 [ 50%]  (Sampling) 
Chain 2 Iteration: 3000 / 6000 [ 50%]  (Sampling) 
Chain 4 Iteration: 3000 / 6000 [ 50%]  (Sampling) 
Chain 1 Iteration: 3100 / 6000 [ 51%]  (Sampling) 
Chain 2 Iteration: 3100 / 6000 [ 51%]  (Sampling) 
Chain 3 Iteration: 3100 / 6000 [ 51%]  (Sampling) 
Chain 4 Iteration: 3100 / 6000 [ 51%]  (Sampling) 
Chain 1 Iteration: 3200 / 6000 [ 53%]  (Sampling) 
Chain 2 Iteration: 3200 / 6000 [ 53%]  (Sampling) 
Chain 3 Iteration: 3200 / 6000 [ 53%]  (Sampling) 
Chain 4 Iteration: 3200 / 6000 [ 53%]  (Sampling) 
Chain 3 Iteration: 3300 / 6000 [ 55%]  (Sampling) 
Chain 1 Iteration: 3300 / 6000 [ 55%]  (Sampling) 
Chain 2 Iteration: 3300 / 6000 [ 55%]  (Sampling) 
Chain 4 Iteration: 3300 / 6000 [ 55%]  (Sampling) 
Chain 3 Iteration: 3400 / 6000 [ 56%]  (Sampling) 
Chain 1 Iteration: 3400 / 6000 [ 56%]  (Sampling) 
Chain 2 Iteration: 3400 / 6000 [ 56%]  (Sampling) 
Chain 4 Iteration: 3400 / 6000 [ 56%]  (Sampling) 
Chain 1 Iteration: 3500 / 6000 [ 58%]  (Sampling) 
Chain 3 Iteration: 3500 / 6000 [ 58%]  (Sampling) 
Chain 2 Iteration: 3500 / 6000 [ 58%]  (Sampling) 
Chain 4 Iteration: 3500 / 6000 [ 58%]  (Sampling) 
Chain 1 Iteration: 3600 / 6000 [ 60%]  (Sampling) 
Chain 3 Iteration: 3600 / 6000 [ 60%]  (Sampling) 
Chain 2 Iteration: 3600 / 6000 [ 60%]  (Sampling) 
Chain 4 Iteration: 3600 / 6000 [ 60%]  (Sampling) 
Chain 1 Iteration: 3700 / 6000 [ 61%]  (Sampling) 
Chain 3 Iteration: 3700 / 6000 [ 61%]  (Sampling) 
Chain 2 Iteration: 3700 / 6000 [ 61%]  (Sampling) 
Chain 4 Iteration: 3700 / 6000 [ 61%]  (Sampling) 
Chain 1 Iteration: 3800 / 6000 [ 63%]  (Sampling) 
Chain 3 Iteration: 3800 / 6000 [ 63%]  (Sampling) 
Chain 2 Iteration: 3800 / 6000 [ 63%]  (Sampling) 
Chain 4 Iteration: 3800 / 6000 [ 63%]  (Sampling) 
Chain 1 Iteration: 3900 / 6000 [ 65%]  (Sampling) 
Chain 3 Iteration: 3900 / 6000 [ 65%]  (Sampling) 
Chain 2 Iteration: 3900 / 6000 [ 65%]  (Sampling) 
Chain 4 Iteration: 3900 / 6000 [ 65%]  (Sampling) 
Chain 3 Iteration: 4000 / 6000 [ 66%]  (Sampling) 
Chain 1 Iteration: 4000 / 6000 [ 66%]  (Sampling) 
Chain 2 Iteration: 4000 / 6000 [ 66%]  (Sampling) 
Chain 4 Iteration: 4000 / 6000 [ 66%]  (Sampling) 
Chain 3 Iteration: 4100 / 6000 [ 68%]  (Sampling) 
Chain 1 Iteration: 4100 / 6000 [ 68%]  (Sampling) 
Chain 2 Iteration: 4100 / 6000 [ 68%]  (Sampling) 
Chain 4 Iteration: 4100 / 6000 [ 68%]  (Sampling) 
Chain 3 Iteration: 4200 / 6000 [ 70%]  (Sampling) 
Chain 1 Iteration: 4200 / 6000 [ 70%]  (Sampling) 
Chain 2 Iteration: 4200 / 6000 [ 70%]  (Sampling) 
Chain 4 Iteration: 4200 / 6000 [ 70%]  (Sampling) 
Chain 1 Iteration: 4300 / 6000 [ 71%]  (Sampling) 
Chain 3 Iteration: 4300 / 6000 [ 71%]  (Sampling) 
Chain 2 Iteration: 4300 / 6000 [ 71%]  (Sampling) 
Chain 4 Iteration: 4300 / 6000 [ 71%]  (Sampling) 
Chain 1 Iteration: 4400 / 6000 [ 73%]  (Sampling) 
Chain 3 Iteration: 4400 / 6000 [ 73%]  (Sampling) 
Chain 2 Iteration: 4400 / 6000 [ 73%]  (Sampling) 
Chain 4 Iteration: 4400 / 6000 [ 73%]  (Sampling) 
Chain 1 Iteration: 4500 / 6000 [ 75%]  (Sampling) 
Chain 3 Iteration: 4500 / 6000 [ 75%]  (Sampling) 
Chain 2 Iteration: 4500 / 6000 [ 75%]  (Sampling) 
Chain 4 Iteration: 4500 / 6000 [ 75%]  (Sampling) 
Chain 1 Iteration: 4600 / 6000 [ 76%]  (Sampling) 
Chain 3 Iteration: 4600 / 6000 [ 76%]  (Sampling) 
Chain 4 Iteration: 4600 / 6000 [ 76%]  (Sampling) 
Chain 2 Iteration: 4600 / 6000 [ 76%]  (Sampling) 
Chain 1 Iteration: 4700 / 6000 [ 78%]  (Sampling) 
Chain 3 Iteration: 4700 / 6000 [ 78%]  (Sampling) 
Chain 2 Iteration: 4700 / 6000 [ 78%]  (Sampling) 
Chain 4 Iteration: 4700 / 6000 [ 78%]  (Sampling) 
Chain 1 Iteration: 4800 / 6000 [ 80%]  (Sampling) 
Chain 3 Iteration: 4800 / 6000 [ 80%]  (Sampling) 
Chain 2 Iteration: 4800 / 6000 [ 80%]  (Sampling) 
Chain 4 Iteration: 4800 / 6000 [ 80%]  (Sampling) 
Chain 1 Iteration: 4900 / 6000 [ 81%]  (Sampling) 
Chain 3 Iteration: 4900 / 6000 [ 81%]  (Sampling) 
Chain 2 Iteration: 4900 / 6000 [ 81%]  (Sampling) 
Chain 4 Iteration: 4900 / 6000 [ 81%]  (Sampling) 
Chain 1 Iteration: 5000 / 6000 [ 83%]  (Sampling) 
Chain 3 Iteration: 5000 / 6000 [ 83%]  (Sampling) 
Chain 2 Iteration: 5000 / 6000 [ 83%]  (Sampling) 
Chain 4 Iteration: 5000 / 6000 [ 83%]  (Sampling) 
Chain 1 Iteration: 5100 / 6000 [ 85%]  (Sampling) 
Chain 3 Iteration: 5100 / 6000 [ 85%]  (Sampling) 
Chain 2 Iteration: 5100 / 6000 [ 85%]  (Sampling) 
Chain 4 Iteration: 5100 / 6000 [ 85%]  (Sampling) 
Chain 1 Iteration: 5200 / 6000 [ 86%]  (Sampling) 
Chain 3 Iteration: 5200 / 6000 [ 86%]  (Sampling) 
Chain 2 Iteration: 5200 / 6000 [ 86%]  (Sampling) 
Chain 4 Iteration: 5200 / 6000 [ 86%]  (Sampling) 
Chain 1 Iteration: 5300 / 6000 [ 88%]  (Sampling) 
Chain 3 Iteration: 5300 / 6000 [ 88%]  (Sampling) 
Chain 2 Iteration: 5300 / 6000 [ 88%]  (Sampling) 
Chain 4 Iteration: 5300 / 6000 [ 88%]  (Sampling) 
Chain 1 Iteration: 5400 / 6000 [ 90%]  (Sampling) 
Chain 3 Iteration: 5400 / 6000 [ 90%]  (Sampling) 
Chain 2 Iteration: 5400 / 6000 [ 90%]  (Sampling) 
Chain 4 Iteration: 5400 / 6000 [ 90%]  (Sampling) 
Chain 1 Iteration: 5500 / 6000 [ 91%]  (Sampling) 
Chain 3 Iteration: 5500 / 6000 [ 91%]  (Sampling) 
Chain 2 Iteration: 5500 / 6000 [ 91%]  (Sampling) 
Chain 4 Iteration: 5500 / 6000 [ 91%]  (Sampling) 
Chain 1 Iteration: 5600 / 6000 [ 93%]  (Sampling) 
Chain 3 Iteration: 5600 / 6000 [ 93%]  (Sampling) 
Chain 2 Iteration: 5600 / 6000 [ 93%]  (Sampling) 
Chain 4 Iteration: 5600 / 6000 [ 93%]  (Sampling) 
Chain 1 Iteration: 5700 / 6000 [ 95%]  (Sampling) 
Chain 3 Iteration: 5700 / 6000 [ 95%]  (Sampling) 
Chain 2 Iteration: 5700 / 6000 [ 95%]  (Sampling) 
Chain 4 Iteration: 5700 / 6000 [ 95%]  (Sampling) 
Chain 1 Iteration: 5800 / 6000 [ 96%]  (Sampling) 
Chain 3 Iteration: 5800 / 6000 [ 96%]  (Sampling) 
Chain 2 Iteration: 5800 / 6000 [ 96%]  (Sampling) 
Chain 4 Iteration: 5800 / 6000 [ 96%]  (Sampling) 
Chain 1 Iteration: 5900 / 6000 [ 98%]  (Sampling) 
Chain 3 Iteration: 5900 / 6000 [ 98%]  (Sampling) 
Chain 2 Iteration: 5900 / 6000 [ 98%]  (Sampling) 
Chain 4 Iteration: 5900 / 6000 [ 98%]  (Sampling) 
Chain 1 Iteration: 6000 / 6000 [100%]  (Sampling) 
Chain 1 finished in 172.6 seconds.
Chain 3 Iteration: 6000 / 6000 [100%]  (Sampling) 
Chain 3 finished in 172.7 seconds.
Chain 4 Iteration: 6000 / 6000 [100%]  (Sampling) 
Chain 2 Iteration: 6000 / 6000 [100%]  (Sampling) 
Chain 2 finished in 173.0 seconds.
Chain 4 finished in 172.9 seconds.

All 4 chains finished successfully.
Mean chain execution time: 172.8 seconds.
Total execution time: 173.2 seconds.
\end{verbatim}

\pagebreak

\section{Disclosures}\label{disclosures}

\subsection{AI Prompts:}\label{ai-prompts}

\subsubsection{Initial Prompt}\label{initial-prompt}

Background:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  There are 3 data frames each formatted like columns {[}outcome,
  intercept, predictors \ldots{]}.
\item
  The outcome Y is always positive.
\item
  The goal is the determine which predictors have non-zero marginal
  effects.
\end{enumerate}

Requirements:

Use stan to fit hierarchical regression models.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  For the likelihood, test a log-normal, a gamma, and a weibull. Note
  that we need to model the mean or a simple function of the mean.
  Ensure that the marginal effects can be put on the same scale as the
  outcome.
\item
  For the priors on the predictor marginal effects (betas) test
  Normal(0, 1), Normal(0, 10), and Normal(0, 100).
\item
  For each model report:

  Table 1:

\begin{verbatim}
 1. Point estimate and 95% credible intervals for each predictor effect. 
 2. The R hat convergence statistics.  
 3. Effect sample size divided by the number of draws.  
\end{verbatim}

  Order parameters first based on absolute distance from zero then by
  interval width ie the parameters with point estimates far from zero
  and/or short intervals should appear higher in the table.

  Table 2:\\
  In different rows, report the quartile values from:

\begin{verbatim}
 1. Raw outcome data.   
 2. Prior predictive distribution. 
 3. Posterior predictive distribution. 
\end{verbatim}

  Table 3: Same format as table 1, but for all non predictor parameters
  eg the hyper- priors

  Print any warnings given by Stan during the fit.
\end{enumerate}

\begin{itemize}
\item
  The final output should be knit quarto pdf.
\item
  Use tools like \texttt{r-targets}, \texttt{target\ markdown}, and
  \texttt{stan\ target}.
\item
  Note that our HPC uses slurm.
\end{itemize}

Ask any clarifying questions before starting.

\subsubsection{Follow-up Prompt}\label{follow-up-prompt}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  No grouping variable, but hyper-priors like this:\\
  \begin{equation*}
  \begin{aligned} 
   \mu_i &= X_i \beta \\ 
   \sigma_i &= a \cdot \mu_i^b \\
   \beta &\sim \mathcal{N}(0, 100) \\
   a &\sim \text{Gamma}(c, d) \\ 
   b &\sim \mathcal{N}(0, 10)
  \end{aligned}
  \end{equation*} We would like to allow for over dispersion.
\item
\end{enumerate}

\begin{itemize}
\item
  The output document should have a distinct section for each dataframe.
  Within a given dataframes section report Table 1 - 3 per likelihood.
  Group columns under each prior scenario.
\item
  Change table 2 to be 3 columns per prior scenario under this grouping
  scheme.
\item
  Order the table based on the most disperse prior ie N(0, 100).
\item
  The non-predictor parameters are going to be stuff like a, b, c, d in
  point 1.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
\end{enumerate}

\begin{itemize}
\tightlist
\item
  I plan to use rstan.\\
\item
  I prefer .sbatch script and slurm job arrays.
\item
  Don't worry about specific MCMC parameters at this point. We need to
  confirm the models fit before a long run.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Draft \_targets.R from scratch.
\item
  I want a single report.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
\end{enumerate}

\begin{itemize}
\tightlist
\item
  We have 3 csv files: df\_1.csv, df\_2.csv, and df\_3.csv
\item
  Data is already clean.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Yes. Put all result in a single qmd.
\item
  I like to use kableExtra, but I can handle formatting. Focus on create
  the appropriate dataframe for each table.
\end{itemize}




\end{document}
